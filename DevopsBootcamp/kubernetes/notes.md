# Kubernetes

- deployment: stateless apps
- statefulSet: stateful apps or dbs

- __worker node:__
- container runtime:
  - also known as container engine
  - it is a software component that can run containers on a host operating system
  - It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.
- kubelet:
  - interacts with both the container and the node
  - starts the pod with the container inside
  - assigns resources from the node to the container like CPU, RAM, and storage resources
- kube-proxy:
  - forwards the requests
- __control Plane:__
- API server:
  - cluster gateway that gets any initial requests of any updates into the cluster, or queries from the cluster
  - acts as a gatekeeper for authentication
- scheduler:
  - after the API server authenticates the request, it forwards it to the scheduler
  - scheduler decides which specific worker node would be able to handle the request of a new pod, component
  - it first looks at your request and sees how many resources the application needs and then matches it with a node that can handle it
  - it only decides on which node the new component should be scheduled, the actual scheduling is done by the kubelet
- controller manager:
  - detects state changes, like crashing pods, and tries to recover the cluster state as soon as possible.
  - it makes a request to the scheduler.
- etcd:
  - a key value store for the cluster state.
  - it is the cluster brain
  - cluster changes, like a crashing pod or a scheduled pod, get stored in the key-value store.
  - the other control plane processes consult it to compare the desired state of the cluster to the present state and therefore try to correct it to achieve the desired state which is stored in the etcd
  - it does not store application data

- kubectl: command line tool for k8s cluster
- install minikube:
  - <https://minikube.sigs.k8s.io/docs/start>
  - `homebrew install minikube`
  - `minikube start --driver docker`
  - `minikube status`

- __Basic kubectl commands:__
  - `kubectl get nodes`
  - `kubectl get pod` or `kubectl get po`
    - ContainerCreating: the pod is created but the pod inside is not yet started
  - `kubectl get services`
  - `kubectl create deployment NAME --image=image --[command] [args...] [options]`
    - `kubectl create deployment nginx-depl --image=nginx`
  - `kubectl get deploy` or `kubectl get deployment`
    - when you create a deployment, the deployment is the blueprint for creating pods
  - `kubectl get replicaset` or `kubectl get rs`
    - RS manages the replicas of a pod.
      - layers of abstraction:
      - deployment manages a replica set, which manages all the replicas of a pod
      - pods are an abstraction of the container
      - everything below a deployment is managed by k8s
  - `kubectl edit deployment nginx-depl` : it produces an auto generated configuration file with default values that you can edit. As soon as you edit and save the changes in the file, the old pod will be killed and a new one started.
  - `kubectl logs [pod name]`
  - `kubectl describe pod [pod name]`
  - `kubectl exec -it [pod name] -- bin/bash`: enters the pod as a root user
  - `kubectl delete deployment [deploy-name] [deploy-name] ...`
  - `kubectl apply -f [file name]`
  - `kubectl delete -f [file name]`
  - each configuration file has 3 parts:
    - metadata
    - specification
    - status: autogenerated by k8s
  - `kubectl get deployment nginx-depl -o yaml > nginx-depl.yaml` : will direct the result into a file instead of stdout.
  - `kubectl get service` or `kubectl get svc`
  - `kubectl get pod -o wide`: get more information about the pod, including its IP
  - `kubectl get all` : get all components
    - `kubectl get all | grep mongodb`: filter the component you want

- __Secret Configuration file:__
  - kind: Secret
  - metadata/name: a name for your secret
  - type:"opaque"- default for arbitrary key-value pairs
  - data: the actual contents- in key-value pairs
- `echo -n 'username' | base64` will encode plain text 'username' with base64
- `echo -n 'dXNlcm5hbWU=' | base64 -d`: decode base64 text
- the secret has to be created before the deployment for it to be referenced in the deployment using `secretKeyRef`.

- __ConfigMap:__
  - configMap, just like secret, has to be created in the cluster before referencing it in a deployment
  - it is referenced as `configMapKeyRef` in the deployment file

- Types of Service in Kubernetes: <https://kubernetes.io/docs/concepts/services-networking/service/>
  - ClusterIP: or internal service. This is the default service if the type of service is not specified.
    - exposes the service on a cluster-internal IP and makes the service only reachable from within the cluster
  - LoadBalancer: will also give service an internal IP address.
    - But in addition, it will also give the service an external IP address where the external requests will be coming from
    - in minikube the external IP is not automatically created.
      - `minikube service mongo-express-service` will assign the external service a public IP address
  - NodePort: port range: must be between ports 30000 and 32767
  - ExternalName:

- __Namespaces:__
  - `kubectl get namespace` or `kubectl get ns`

    ``` kubernetes default namespaces
    NAME              STATUS   AGE
    default           Active   2d21h
    kube-node-lease   Active   2d21h
    kube-public       Active   2d21h
    kube-system       Active   2d21h
    ```

  - kube-system:
    - Do not create or modify in this namespace.
    - components deployed here are the system processes from the control plane or kubectl processes
  - kube-public:
    - contains publicly accessible data
    - it has a configmap that contains cluster information which is accessible even without authentication
    - `kubectl cluster-info`
  - kube-node-lease:
    - it holds information about the heartbeats of nodes.
    - so each node basically gets its own lease object that contains information about that node's availability
  - default:
    - You can create resources in this namespace(default, of no other namespace defined) or you can create your own namespace
    - `kubectl create namespace <my-namespace>`
    - you can also create the namespace using a configuration file:

    ```namespace configuration file:
    apiVersion: v1
    kind: Namespace
    metadata: my-namespace
    ```

  - __Why namespaces:__
    - Structure your components: group resources into namespaces.
    - Avoid conflicts between teams, one application: Use namespaces to avoid one team overwriting the other's application with configuration changes
    - Share services between different environments. resource sharing: consider staging and development. namespaces give the ability to reuse components in both environments or in a blue-green deployment
    - access and resource limits on namespace level. Give teams access to their own namespaces and restrict their access to other namespaces where other teams are working. You can also limit the resources like CPU and RAM that one namespace can use
  - __characteristics of a namespace:__
  - You cannot access most resources from another namespace.
    - you would have to define configmaps and secrets for each namespace if they are sharing the same resource.
    - service can be shared across namespaces

    ```access service in another namespace:
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: mongodb-configmap
      namespace: my-namespace
    data:
      database_url: mongodb-service.database  #service.namespace
    ```

  - There are some components that can't be created within a namespace because they live globally on a cluster and cannot be isolated.
    - e.g. volume or persistent volume and node
    - `kubectl api-resources --namespaced=false`: to list all resources that cannot be namespaced
    - `kubectl api-resources --namespaced=true`
  - you can change the active namespace with kubectl:
    - `kubectl config set-context --current --namespace-my-namepsace`
  - you can also use kubens: more convenient, but needs to be installed separately.
  - `brew install kubectx`
    - alternatively, if you do not want to install kubectx, you can create an alias in your .bashrc/.zshrc to get the same functionality:
      - `alias kubectx="kubectl config use-context"`
  - How to use: <https://github.com/ahmetb/kubectx>

- __Services:__
  - each pod gets its own IP address, but pods are ephemeral. Service has a stable IP address that is not linked to the pod lifecycle
  - it also provides load balancing to pods of the same application/replicas
  - they are a good abstraction of loose coupling for communication within and outside of the cluster
  - a service identifies its member pods or its endpoint pods using the selectors attribute.
  - when you have a pod with two ports, the service knows which port to direct the request to using the targetPort attribute.
  - when you create a service, k8s creates an endpoint object that has the same name as the service itself. K8s will use this endpoints object to keep track of which pods are members/endpoints of the service.
  - service communication: port vs. targetPort-
    - service port is arbitrary
    - targetPort must match the port the container/ application port inside the container is listening at
  - 3 service type attributes: ClusterIp, NodePort, LoadBalancer
  - __ClusterIP:__
    - default service type if no other service is specified.
    - Only accessible within the cluster.
    - Multiport services:
      - when you have multiple ports defined in a service, you have to name those ports, while this is optional in services with only one port.

      ```multiport-service
      apiversion: v1
      kind: Service
      metadata:
        name: mongodb-service
        ...
      spec:
        selector:
          app: mongodb
        ports:
          - name: mongodb
            protocol: TCP
            port: 27017
            targetPort: 27017
          - name: mongodb-exporter
            protocol: TCP
            port: 9216
            targetPort: 9216
      ```

    - Headless Service:
      - used when the client/pod wants to communicate with one specific pod directly, instead of a randomly selected pod.
      - Use case: stateful applications like databases
      - client needs to figure out IP addresses of each pod.
        - Option 1: API call to k8s API server and it will return the list of pods and their IP addresses.
          - This makes the app too tied to k8s api and inefficient because you will have to list the pods and their IP addresses everytime you want to connect to the pods
        - Option 2: DNS lookup
          - As an alternative, k8s allows the client to discover pod IP addresses through DNS lookups. When a client performs a DNS lookup for a service, the DNS server returns a single IP address which belongs to the service (clusterIP address).
      - Option 3: Headless service:
        - however, if you tell k8s that you do not need a cluster IP address of the service by setting the clusterIP field to none when creating a service, then the DNS server will return the pod IP addresses instead of the services IP address. And now the client can do a simple DNS lookup to get the IP address of the pods that are members of that service. and then the client can use that IP address to connect to the specific pod it want to talk to or all the pods.

      ```headless service
      apiVersion: v1
      kind: Service
      metadata:
        name: mongo-service-headless
      spec:
        clusterIP: None
        selector:
          app: mongodb
        ports:
          - protocol: TCP
            port: 27017
            targetPort: 27017
      ```

  - __NodePort:__
    - creates a service that is accessible on a static port on each worker node in the cluster.
    - makes the external traffic accessible on the static port on each worker node
    - nodeport value has a predefined range between 30000 and 32767
    - when you create a nodePort service, a cluster Ip is also automatically created.
    - not secure because nodeport service opens the port to directly talk to services on each worker node, so the external clients have access to worker nodes directly.
    - nodeport is not for use in production envs; configure Ingress or LoadBalancer for prod envs.

  - __LoadBalancer:__
    - service is made accessible externally through a cloud provider's load balancer functionality.
    - Whenever we create a load balancer service, nodeport and cluster Ip services are created automatically by k8s, to which the external load balancer of the cloud platform will route the traffic to.
    - loadbalancer service is an extension of NodePort service
    - nodeport service is an extension of clusterIp service
    - Difference between Load Balancer and Ingress: <https://www.baeldung.com/ops/kubernetes-ingress-vs-load-balancer#:~:text=While%20ingresses%20and%20load%20balancers,route%20to%20a%20single%20service.>
      - While ingresses and load balancers have a lot of overlap in functionality, they behave differently.
      - The main difference is ingresses are native objects inside the cluster that can route to multiple services,
        while load balancers are external to the cluster and only route to a single service.

- __Ingress:__
  - List of Ingress Controllers you can choose from: <https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>
  - external service vs. ingress:
    - Ingress does not work with an external service, it does with an internal service. This means that you don't have to open the application via IP address and port number. Now, if the request arrives from the browser, it is received by the Ingress and then it directs it to the desired service and eventually ends up with the Pod. <https://medium.com/@owaisnasir433/kubernetes-ingress-vs-external-services-c1b61acb9c78#:~:text=To%20differentiate%20more%20clearly%2C%20Ingress,IP%20address%20and%20port%20number.>
    - An ingress is really just a set of rules to pass to a controller that is listening for them. You can deploy a bunch of ingress rules, but nothing will happen unless you have a controller that can process them. A LoadBalancer service could listen for ingress rules, if it is configured to do so.
    - An Ingress Controller is simply a pod that is configured to interpret/ evaluate and process ingress rules and manages all the redirections. One of the most popular ingress controllers supported by kubernetes is nginx. Without it, the ingress would not work.
    - external service:

    ```externalservice.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: myapp-external-service
    spec:
      selector:
        app: myapp
      type: LoadBalancer
      ports:
        - protocol: TCP
          port: 8080
          targetPort: 8080
          nodePort: 35010
    ```

    - ingress:

    ```ingress.yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: myapp-ingress
    spec:
      rules:
      - host: myapp.com
        http:
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: myapp-internal-service
                  port:
                    number: 8080
    ```

    - routing rules: forward request to the internal service
    - backend: where the incoming request will be directed and the service name should correspond to the internal service name
    - port: internal service port
    - host has to be a valid domain address
      - map the domain name to the IP address, which is the entrypoint to your k8s cluster
  - you can configure the ingress in different ways:
    - using a cloud-provider load balancer which then forwards the request to the internal service
    - using a proxy-server: This is a separate server with a public IP address and open ports which acts as the entrypoint to the cluster, and none of the servers in k8s cluster is accessible from outside
- install ingress controller in minikube:
  - `minikube addons enable ingress`
  - this automatically starts the k8s nginx implementation of ingress controller
  - minikube uses minikube tunnel to enable ingress access to the resources
- create ingress rule
  - enable minikube dashboard:
    - `minikube dashboard`
  - configure an ingress rule for the dashboard to it is available externally using a domain name
    - `kubectl get all -n kubernetes-dashboard`
    - `kubectl get ingress -n kubernetes-dashboard`
  - map the localhost address to dashboard.com(minikube)
    - 127.0.0.1 dashboard.com
    - `minikube tunnel`: once you close the tunnel, the application will no longer be accessible on the browser although it is configured in the /etc/hosts file
    - now you can access the kubernetes dashboard at dashboard.com
- ingress default backend:
  - `kubectl describe ingress -n kubernetes-dashboard`
  - whenever a request comes into the cluster that is not mapped to any backend, so there is no rule for mapping that request to a service, then this default backend is used to handle that request.
  - a good usage for that is to define custom error messages when a page isn't found, when a request comes in that the application cannot handle so that the user still sees a meaningful error message or a custom page where you can redirect them to your home page, etc.

  ```default backend
  spec:
    defaultBackend:
      service:
        name: kubernetes-dashboard
        port:
          number: 80
  ```

- more use cases:
  - defining multiple paths for the same host
    - 1 domain, many services
    - accessible at myapp.com/analytics and myapp.com/shopping

  ```multiple-paths-ingress.yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: simple-fanout-example
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
  spec:
    rules:
    - host: myapp.com
      http:
        paths:
        - pathType: Prefix
          path: /analytics
          backend:
            service:
              name: analytics-service
              port:
                number: 3000
        - pathType: Prefix
          path: /shopping
          backend:
            service:
              name: shopping-service
              port:
                number: 8080
  ```

- multiple sub-domains or domains:
  - instead of one host and multiple paths, this one has multiple hosts with one path, where each host represents a subdomain and inside each subdomain, there is one path that again redirects the requests to the service in question
  - available at analytics.myapp.com and shopping.myapp.com

  ```multiple-subdomains.yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: name-virtual-host-ingress
  spec:
    rules:
    - host: analytics.myapp.com
      http:
        paths:
        - path: /
          backend:
            service:
              name: analytics-service
              port: 
                number: 3000
    - host: shopping.myapp.com
      http:
        paths:
        - path: /
          backend:
            service:
              name: shopping-service
              port:
                number: 8080
  ```

- configuring a TLS certificate- https://
  - configure a tls attribute under specification:

  ```tls-configuration
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: tls-example-ingress
  spec:
    tls:
    - hosts:
      - myapp.com
      secretName: myapp-secret-tls
    rules:
    - host: myapp.com
      http:
        paths:
        - path: /analytics
          backend:
            service:
              name: myapp-internal-service
              port: 
                number: 8080
  ```

tls-certificate:

```tls-certificate
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secret-tls
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
```

- to note:
  - data keys need to be "tls.crt" and 'tls.key"
  - values are file contents, not file paths/locations
  - secret component has to be created in the same namespace as the ingress component for it to be able to use the secret, otherwise you cannot reference the secret from another namespace

- __Volumes:__
  - we persist data in k8s using volumes
  - there are different types of volumes available:
    - persistent volume types: exist beyond the lifetime of a pod
    - ephemeral volume types: used to persist data in memory. k8s volume destroys volume when pod ceases to exist.
      - that volume type is called *emptyDir* or empty directory
      - starts as an empty dir and is first created when a pod is assigned to a node- references that empty directory, and exists as long as the pod is running
      - so, when the pod dies and is scheduled on another node, all the data in the emptyDir will be deleted permanently
      - *To Note*: container crashing does NOT remove a pod from a node, therefore the data stays safe across container crashes
  - persistent volume: storage needs to not depend on the pod lifecycle.
    - storage also needs to be available to all nodes, not just a specific node because we cannot determine the node that the pod will be scheduled on, once it restarts.
    - storage needs to survive even if the cluster crashes
    - persistent volume: A persistent volume is a piece of storage in a cluster that an administrator has provisioned. It is a resource in the cluster, just as a node is a cluster resource. A persistent volume is a volume plug-in that has a lifecycle independent of any individual pod that uses the persistent volume.<https://www.netapp.com/devops-solutions/what-is-kubernetes-persistent-volumes/#:~:text=A%20persistent%20volume%20is%20a,that%20uses%20the%20persistent%20volume.>
    - PersistentVolume(pv) is just an abstract component and it takes the storage from the actual physical storage like the local hard drive from the cluster nodes, or external NFS servers outside the cluster, cloud storage like AWS block storage, Google cloud storage, etc.
    - you need to create and manage the storage because kubernetes only provides the pv component

  ```pv.yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: mypv
  spec:
    capacity:
      storage: 5Gi
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: slow
    mountOptions:
      - hard
      - nfsvers=4.1
    nfs:
      path: /dir/path/on/nfs/server
      server: nfs-server-ip-address
  ```

- google cloud example:

```gcp-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-volume
  labels:
    topology.kubernetes.io/zone: us-central1-a__us-central1-b
spec:
  capacity:
    storage: 400Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
```

- spec attributes differ according to storage type.
- local storage example:

  ```local-storage.yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-name
  spec:
    capacity:
      storage: 100Gi
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: slow
    local:
      path: /mnt/disks/ssd1
    nodeAffinity:
      required:
        nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - example-node
  ```

  - persistent volumes are not namespaced and are available to the whole cluster
  - local vs. remote volume types:
    - local volume types violate requirements for data persistence: being tied to 1 specific node and surviving cluster crashes.
    - you should almost always use remote storage for best practices.
    - kubernetes supports several types of volumes: <https://kubernetes.io/docs/concepts/storage/volumes/#volume-types>
  - pv resources need to be created before the pod referencing the PV is created.
  - The application has to claim that persistent volume storage using a persistent volume claim (pvc).
    - example of pvc:

  ```pvc.yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: pvc-name
  spec:
    resources:
      requests:
        storage: 10Gi
    storageClassName: manual
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
  ```

- use that pvc in pod configuration:

```pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: myfrontend
    image: nginx
    volumeMounts: #container
    - mountPath: "/var/www/html"
      name: mypd
  volumes: # pod
    - name: mypd
      persistentVolumeClaim:
        claimName: pvc-name
```

- levels of volume abstractions:
  - pod requests the volume through the pv claim
  - pvc tries to find a volume in the cluster
  - pv has the actual storage backend that it will create that storage from
  - claims must exist in the same namespace as the pods using the pvc, while the pv is not namespaced.
  - Once the pod finds the matching pv, through the pvc, the volume is then mounted into the pod, then the volume is mounted into the container.
  - if you have multiple containers, then you can choose which containers to mount the volume into
- Both configMap and secrets are local volumes that are not create via pv and pvc, but rather own components that are managed by kubernetes itself
- Storage Class:
  - SC provisions persistent volumes dynamically whenever PVC claims it.
    - it creates persistent volumes in the background
  - storage-class example:

``` storageClass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: storage-class-name
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGb: "10"
  fsType: ext4
```

- Each storage backend has its own provisioner.
  - internal provisioner: prefixed with 'kubernetes.io'
  - external provisioner
- storage class usage:
  - requested by persistent volume claim
    - pod claims storage via pvc
    - pvc requests storage from SC
    - SC creates pv that meets the need of the claim

- __ConfigMap and Secret as k8s volumes:__
  - you can create the configmap and secrets as files that can be mounted into the container so that the application in that container can access them.
  
  ```configmap
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: mosquitto-config-file
  data:
    mosquitto.conf: | # file name and below file contents
      log_dest stdout 
      log_type all
      log_timestamp true
      listener 9001
  ```

- secret:

  ```secret
  apiVersion: v1
  kind: Secret
  metadata:
    name: mosquitto-secret-file
  type: Opaque
  data:
    secret.file: |
      ghjJKseyh3CdJLjmn4k= #base64 code value
  ```

- certificate:

```Certificate
  apiVersion: v1
  kind: Secret
  metadata:
    name: my-secret
  type: Opaque
  data:
    cacert.pem: |
      base64 code value of a PEM certificate
```

- code:
  - starting code: <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/configmap-and-secret-volume-types/-/tree/starting-code>
  - final code: <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/configmap-and-secret-volume-types>
  - `kubectl exec -it mosquitto-746f66ff55-kwh9g  -- /bin/sh`
  - overwrite the mosquitto.conf file using the configmap by mounting it into the container
  - configmap and secret must be created and exist before the pod starts in the cluster
  - `kubectl get secret`
  - `kubectl get configmap` or `kubectl get cm`
  - mountPath refers to the path in the file system inside the container

- __statefulSet:__
  - it is a kubernetes component that is used specifically for stateful applications, eg. all databases or any application that stores data to track the state by saving the data in some storage.
  - stateless applications are deployed using deployment; stateful applications are deployed using the statefulset component.
  - statefulset makes it possible to replicate the statefulSet app pods.
  - Replica pods are not identical- pod identity. StatefulSet maintains a sticky identity for each of its pods. While the pods are created from the same specification they are not interchangeable.
  - each has a persistent pod identifier that it maintains across any scheduling.
  - the db pods do not have the same physical storage although they use the same data; they each have replicas of the storage that each of them can access for themselves. This means that at any given time that each pod replica must have the same data as the other replicas, so they have to continuously synchronize their data. only the main is allowed to change the data and the replicas must know of each change and update their own storage.
  - when a new db pod joins the cluster, it clones the data from not just any pod, but the previous pod, and once that is done and its up to date, it starts continuous synchronization as well to listen for any updates from by the master pod.
  - configure persistent volumes for your statefulset so the data can survive pod and node crashes
  - pod identity: while deployments get a random hash , statefulSet gets a fixed ordered names which is made up of the statefulset name and an ordinal that starts from zero and each additional pod gets the next numeral: $(statefulset name)-$(ordinal), eg mysql-0 (main), mysql-1 ... (replicas)
  - StatefulSet will not create the nextpod in the replica if the previous one is not already up and running. if the first creation fails or is pending, the next replica will not get created at all, it would just wait.
  - the same order is held during deletion, but in reversed order and will wait until the largest ordinal number replica is deleted, before it can delete the one preceding it.
  - each pod also gets its own DNS endpoint from a service. ${pod name}.${governing service domain} eg. mysql-0.svc2, mysql-1.svc2...
  - these two characteristics ( predicatable pod name and fixed individual DNS name) means that when a pod restarts, the IP address will change but the name and endpoint will stay the same- hence sticky identities.
  - The sticky identity ensures that each replica pod can retain its state and its role even when it dies and gets recreated.
  - replicating stateful apps is complex. While k8s helps some, you still need to configure cloning and data synchronization inside the stateful set and also make the remote storage available as well as take care of managing and backing up the remote storage.

- __Helm__
  - package manager for Kubernetes
  - helm charts: bundle of YAML files
  - search for a helm chart: `helm search <keyword>`
  - you can search for helm charts in public registries like artifacthub.io, bitnami
  - there are also helm charts that are available in private registries that are shared within an organization
  - helm is also a templating engine.
    - define a common blueprint
    - dynamic values are replaced by jinja template placeholders (template file)
  - example template yaml config

  ```template-config.yaml
  apiVersion: v1
  kind: pod
  metadata: 
    name: {{ .Values.name }}
  spec:
    containers:
    - name: {{ .Values.container.name }}
      image: {{ .Values.container.image }}
      port: {{ .Values.container.port }}
  ```

- values.yaml

```values.yaml
  name: my-app
  container:
    name: my-app-container
    image: my-app-image
    port: 9001
```

- .Values is an object that is being created based on  the values that are supplied via values.yaml file and also through the CLI with --set flag.
- helm chart structure:
  - top level: myChart/ folder- name of the chart
  - Chart.yaml: meta info about the chart, eg. name, version, dependencies
  - values.yaml: values for the template files. These will be the default values you can override later
  - charts/ folder: chart dependencies
  - .helmignore: files that you do not want to include in your helm chart
  - templates/ folder: the actual template files or where the template files are stored.
    - when you execute `helm install <chartname>` the template files will be filled with the values from values.yaml
  - you can optionally have other files like README.md or license file, etc
- you can override the default values in the values.yaml file in two ways:
  - by using the --values flag to represent the new values.yaml file
    - `helm install --values=my-values.yaml <chartname>`
    values.yaml(default)

    ```values.yaml
    imageName: myapp
    port: 8080
    version: 1.0.0
    ```
  
    - my-values.yaml

    ```my-values.yaml
    version: 2.0.0
    ```

    - result:

    ``` result:
    imageName: myapp
    port: 8080
    version: 2.0.0
    ```

  - by using the --set flag on the CLI
    - eg. `helm install --set version=2.0.0`

- Release management:
  - managed by the release binary
  - if an upgrade to a Helm chart is released, or if you need to change the configuration of your deployment, you can run `helm upgrade <chartname>`
    - any changes made since the last release are going to be applied to the existing deployment instead of creating a new one
  in case the upgrade was fails or the configuration changes are wrong, you can rollback that upgrade using `helm rollback <chartname>` to specific version of the chart if you add the version to the command
- __installing helm:__
  - <https://helm.sh/docs/intro/install>
    - MacOs: `brew install helm`
  - <https://docs.bitnami.com/kubernetes/infrastructure/kubeapps/get-started/install/>
  - <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/helm-demo>

  - `helm repo add bitnami https://charts.bitnami.com/bitnami`
  - `kubectl cluster-info`
  - `helm search repo bitnami`
  - <https://github.com/bitnami/charts/blob/main/bitnami/mongodb>
  - <https://github.com/bitnami/charts/tree/main/bitnami/mongodb>
  - `helm install mongodb --values helm-mongodb.yaml bitnami/mongodb`
  - to delete a chart: `helm delete <my-RELEASE>` ... `helm delete mongodb`
  - `kubectl get secret mongodb -o yaml`
  - <https://medium.com/google-cloud/kubernetes-storage-overview-of-storage-classes-af1102e7b3f9>
  - `kubectl apply -f helm-mongo-express.yaml`
  - `kubectl get svc`, `kubectl get pod`, `kubectl get deploy`
  - install kubernetes ingress controller- nginx via helm chart:
    - `helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx`
    - `helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true`
    - `controller.publishService.enabled=true` will enable a load balancer
  - create ingress rule for mongo-express:
    - `kubectl apply -f helm-ingress.yaml`
    - `kubectl get ingress`
      - accessed at external-ip-address:80 since I have not configured a host
    - `kubectl scale --replicas=0`
    - `kubectl get statefulset`
    - `kubectl scale --replicas=0 statefulset/mongodb`
    - `helm ls`
    - `helm uninstall <chartname>`

- __deploying images in k8s from private Docker repo:__
  - steps to pull image from private registry
    - create secret component: contains credentials for docker registry
      - `aws ecr get-login-password`
    - ssh into minikube:
      - `minikube ssh`
      - `pwd`, `ls -a`
      - `docker login --username AWS -p <the token you got from the aws ecr command above> <AWS-account-id>.dkr.ecr.ca-central-1.amazonaws.com`
    - use the .docker/config.json to create the secret component:
      - copy the file from minikube to the host:
        - `minikube cp minikube:/home/docker/.docker/config.json ~/.docker/config.json`
        - `cat ~/.docker/config.json | base64` and paste this in docker-secret.yaml. Then apply the docker-secret.yaml file to create the secret
        - Alternatively, skip straight to creating the secret:

        ```secret
        kubectl create secret generic my-registry-key \
        --from-file=.dockerconfigjson=~/.docker/config.json \
        --type=kubernetes.io/dockerconfigjson
        ```

      - you can also create the secret like this:

      ```secret
      kubectl create secret docker-registry my-registry-key-two \
      --docker-server=https://137236605350.dkr.ecr.ca-central-1.amazonaws.com \
      --docker-username=AWS \
      --docker-password=<aws ecr get-login-password> \
      ```

    - configure deployment/pod: use secret using imagePullSecrets
    - apply both deployment files.
      - imagePullAlways forces docker to pull the image from AWS
      - imagePullSecrets: references the secret in docker-secret.yaml file
  - *To note!*: Secret has to be in the same namespace as Deployment/StatefulSet or any other component that's being created that needs to reference it.

- __Kubernetes Operator:__
  - <https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-operators/#:~:text=In%20Kubernetes%2C%20an%20operator%20is,(API)%20and%20kubectl%20tooling.>
  - In Kubernetes, an operator is an application-specific controller that can help you package, deploy, and manage a Kubernetes application.
  - Ordinarily, you run and manage Kubernetes applications via the Kubernetes application programming interface (API) and kubectl tooling. Operators lets you extend the functionality of the Kubernetes API, enabling it to configure, create, and manage instances of applications automatically using a structured process.
  - Operators use the basic capabilities of Kubernetes controllers and resources, but add application-specific or domain knowledge to automate the entire lifecycle of the application it manages.
  - it is a custom control loop in k8s.
  - makes use of (custom resource definitions)CRDs
  - operator takes the basic k8s resources and its controller concept, as a foundation to build upon. on top of that, it includes the domain or application specific knowledge to automate the entire lifecycle of the application it manages or operates
  - while k8s can manage the complete lifecycle of stateless apps, it cannot automate the process natively for stateful apps, and there it uses its own operator for each different application
  - there are already multiple operators that have been created and are ready to use but you can also create your own operator with Operator SDK

- __Role-Based Authorization Control:__
  - least-privilege rule
  - with RBAC, you can define namespaced permissions. it also determines what you can do with those resources (CRUD)
  - RoleBinding: link("bind) a role to a user or group. All members of the group get the permissions defined in the role
  - Cluster role defines what resources have what permissions, cluster-wide. cluster role is not namespaced
  - Kubernetes does not manage users natively. it does not have a k8s object for representing normal user accounts. it relies on external sources for creating and managing users and groups
  - Admins can choose from different authentication strategies.
    - can be a static token file, certificates signed by k8s or a 3rd party identity service like ldap
  - admin configures external source
  - API server handles authentication of all the requests
    - pass token file via `--token-auth-file=/users.csv`
    - `kube-apiserver --token-auth-file=/users.csv [other options]`
  - you can also link a service account to a role with roleBinding or to a clusterRole with clusterRoleBinding
  - example role.yaml

  ```role.yaml
  kind: Role
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    # namespace: default
    name: developer
  rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "create", "list"]
    # resourceNames: ["myapp"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
    # resourceNames: ["mydb"]
  ```

  - apiGroups: "" indicates the core API group
  - resources: k8s components like pods, deployments, etc
  - verbs: the action on a resource
  - namespace: you can define a namespace, otherwise if the namespace is not defined, then the role will be created in the default namespace
  - resourceNames: you can also define access to only certain pods in that namespace using the resourceNames attribute

  - RoleBinding: attaches or binds a subject to a role

  ```rolebinding
  kind: RoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: phyllis-developer-binding
  subjects:
  - kind: User
    name: phyllis
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: Role
    name: developer
    apiGroup: rbac.authorization.k8s.io
  ```
  
  ```group subject
  subjects:
  - kind: Group
    name: "devops-admins"
    apiGroup: rbac.authorization.k8s.io

  ```

  ```serviceaccount subject
  subjects:
  - kind: ServiceAccount
    name: default
    apiGroup: kube-system
  ```

  - ClusterRole and ClusterRoleBinding example:
    - you can define a cluster role for:
      - cluster-wide resources
      - namespaced resources in a clusterRole: grants access to that component in all namespaces
  
  ```clusterrolebinding for cluster-wide resources
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: cluster-admin
  rules:
  - apiGroups: [""]
    resources: ["nodes", "namespaces"]
    verbs: ["get", "create", "list", "delete", "update"]
  ```

  ```clusterrolebinding for namespaced resources
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: cluster-admin
  rules:
  - apiGroups: [""]
    resources: ["pods", "services", "deployments"]
    verbs: ["get", "create", "list", "delete", "update"]
  ```

  - clusterrolebinding:

  ```clusterrolebinding
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: read-secrets-global
  subjects:
  - kind: Group
    name: cluster-admins
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  ```

  - `kubectl get roles`
  - `kubectl describe role <role name>`

  - You can also check the privileges that your current user has using kubectl auth command.
    - kubectl provides `auth can-i` subcommand to check if the current user can perform a given action
      - `kubectl auth can-i create deployments --namespace dev`
    - admins can also check permissions of other users

- __How do microservices communicate?__
  - through interfaces or APIs: a code with functions inside the application itself is responsible for handling communication with other services
  - through a third party message broker application. Instead of each service sending requests to all the others, all microservices talk to the message broker requesting them to forward a request to a certain microservice, creating less code complexity inside the microservices. popular 3rd party message brokers for microservices are Redis, rabbitMQ, pub sub
  - through a service mesh architecture, where instead of one central message broker that handles all the communication, each microservice has its own helper program(sidecar in k8s) that handles the communication for that specific microservice. example: Istio
  - __information you need from developers:__
    - what microservices you need to deploy
    - which microservice is in communication with what microservice
    - how are the communicating? API calls, message broker or Service Mesh?
    - which db are they using? 3rd party services?
    - on which port each microservices runs/listens on

- __Deploy Microservices Application:__
  - Google-code: <https://github.com/phyllisn-landmark/nana-janashia-twn-microservices-demo.git>
  - `kubectl create ns microservices`
  - `kubectl apply -f deployment.yaml -n microservices`
  - `kubectl get pod -n microservices`
  - `kubectl get svc -n microservices`
  - remember to open the firewall on port 30007!

- __Production and Security Best Practices:__
  - __specify a pinned(tag) version for container image for each microservice__

  - __configure a liveness probe on each container__
    - k8s knows to restart a pod when the pod dies. However, we need to let Kubernetes know what state the application that is running inside the container dies, or what state our application is in so that k8s can automatically restart the application.
    - performs health checks with Liveness probe
    - with *liveness probe* defined k8s will automatically restart the pod when the application is crashing or having an issue
    - liveness probe is a container attribute
    - define the attribute depending on what protocol the application uses to return the healthcheck info; http or tcpSocket, gRPC
      - __*tcpSocket*:__ kubelet makes probe connection at the node, not in the pod
        - the kubelet will attempt to open a socket to your container on the port specified, where the application is running. If it succeeds to establish connection on that port, the container is considered to be healthy, so the application is running nad reachable. if it cannot establish that connection, then the application is considered to be failing. So the readiness or liveness probe will fail for the application
        - we can execute __*initialDelaySeconds*__ in case we know that the application takes longer to start. This tells kubelet to wait five seconds before performing the first liveness or readiness probe
      - __*HTTP probes*:__ kubelet sends a HTTP request to specificed path and port.
        - it checks the application health on a HTTP endpoint.
        - e.g. if the app has an endpoint inside the application that exposes the health status of the application itself, we could hit that endpoint to check whether the app is health or not
        - path: the URL of the health endpoint in the application,
    - livenessProbe:

      ```livenessProbe
      livenessProbe:
        grpc:
          port: 8080 # tell it which port to direct the gRPC requests to. It runs in resonse to requests to port 8080
        periodSeconds: 5 # defines how often the endpoint of the app should be checked for its health. time in seconds
      ```

  - __Configure a readiness probe on each container__
    - k8s knows the pod state and the liveness probe helps k8s check that the application is also healthy/ or rather does health checks __*after*__ the container is started.
    - How does k8s know that the app is fully initiated and started and ready to receive requests? This is where the readiness probe comes in.
    - __*Readiness Probe*__ helps kubernetes know that the application is ready to receive traffic
    - without the readiness probe, k8s assumes that the app is ready to receive traffic as soon as the container starts
    - both readiness and liveness probe check for application availability. Readiness probe does it during application startup and liveness probe does it when the application is running

    ```readinessProbe
    readinessProbe:
      grpc:
        port: 8080
      periodSeconds: 5
    ```

  - __Configure resource requests for each container.__
    - there are two types of resources: CPU and Memory
    - Requests is what the container is guaranteed to get.
      - The scheduler uses this to figure out where to run the pods
      - best practice: keep CPU request at "1" or below.
      - cpu resources are defined in millicores=m
      - memory resources are defined in bytes. Mi=mebibytes
  - __Configure resource limits for each container.__
    - what happens if the application needs more resources than it has requested? e.g. when the application has too much data to load into memory or when you have a bug in the application which has an infinite loop that consumes all the CPU available on that node?
      - In this case, the container will consume more than the requested resources and it can take all of the node's resources and cause it to crash, if it is not limited, or take all the resources from other pods on the same node, forcing them to be rescheduled.
    - therefore, we need to make sure the container never goes above a certain value.
    - that container is only allowed to go up to the specified limit
    - __*To Note:*__ if you put values larger than your biggest node, your pod will never be scheduled
  - __Do not expose a NodePort!__
    - it exposes the cluster to a security risk because it opens a port on each worker node in the cluster where it can be directly accessed by external sources.
    - This creates multiple entry points in the cluster, hence increasing the attack surface.
    - the best practice is to only have internal services and only have one entry point to the cluster where all requests come in and ideally that entrypoint should be sitting outside the cluster on a separate server.
    - so instead of a nodeport, we can use a loadbalancer type which uses the platform/s load balancer to create an external single entrypoint for the cluster, which will then redirect the request to the cluster into the internal service which is responsible for that request.
    - as an alternative we can use an ingress and ingress controller to direct traffic to the internal services
  - __Configure more than 1 replica for Deployment.__
    - when you do not configure the number of replicas, the replica is 1 by default
    - if 1 pod crashes, your application is not accessible until a new pod restarts. With more replicas, the application is always available; no downtime for users
  - __Configure more than 1 worker node in your cluster__
    - this helps to avoid a single point of failure that is presented by only having one node.
    - there are several reasons for server unavailability:
      - server crashes, server reboots because of an update, server maintenance...etc.
    - if you have more than one node, then the workloads can be scheduled on another node
  - __Use labels for all resources__
    - it is like giving custom identifiers to your components.
    - the labels should be meaningful and relevant to users
    - example use cases: group pods with labels, reference in service component
    - example technical label:

    ```technical label
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        application: myapp
        version: "v31"
        release: "r42"
        stage: production
    ```

    - example business label

    ```business label
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: my-deploy
      labels:
        owner: payment-team
        project: fraud detection
        business-unit: "80432"
    ```

  - __Use namespaces to isolate your resources from each other__
    - namespaces are also useful in defining access rights/privileges - roles and rolebinding
  - __Ensure that are the images that you are using in your cluster are free of vulnerabilities__
    - this can happen if you are using base images, third-party libraries, or tools that have vulnerabilities
    - do manual vulnerability scans on images or in a build pipeline
  - __No root access for containers.__
    - Configure containers to use unprivileged users
    - example root access!:

    ```root access
    spec:
      containers:
      - name: app
        image: nginx:1.25.2
        securityContext:
          privileged: true
    ```

    - this exposes a security risk because a container with root access can access more resources and do more damage on the host where it's running.
      - with root access, attackers have access to host-level resources.
      - usually official images do not use root user but it is always a good practice to check whether the container is running as root/ operating in root user mode
  - __Update Kubernetes to the latest version.__
    - Each version has important security fixes and general bug fixes
    - To update a k8s cluster, you do this node by node to avoid application and cluster downtime, which is why it's important to have multiple replicas and nodes

- __Helm Chart for Microservices:__
  - Create a helm chart:
    - `helm create microsvc`
    - in the template folders, delete all other files except service.yaml and deployment.yaml
    - clear out the contents in deployment.yaml, service.yaml and values.yaml
  - "Values" object:
    - Values is a built-in object which is by default empty
    - values are passed into template from 3 sources:
      - the values.yaml file
      - user-supplied file with the -f flag
      - parameter passed with --set flag on the CLI
  - Built-in objects:
    - several objects are passed into a template from the template engine.
      - e.g. "Release", "Files", "Values"...
      - <https://helm.sh/docs/chart_template_guide/builtin_objects>
  - variable naming conventions:
    - names should begin with a lowercase letter
    - separated with camelcase. no dashes

  - flat or nested values:
    - values may be flat or nested deeply/hierarchical structure:
    - Best Practice is to use flat structure, which is simpler
      - flat:
      - {{ .Values.appName }}
      - {{ .Values.appReplicas }}

        ```flat
        appName: myapp
        appReplicas: 2
        ```

      - nested:
      - {{ .Values.app.name }}
      - {{ .Values.app.replicas }}

    ```nested
    app:
      name: myapp
      replicas: 1
    ```

  - Dynamic environment variables
    - "range"
      - when working with a list, there is a built-in function in the template files called "range"
      - provides for a "for each"- style loop
      - range is the same thing. it loops/iterates through/ranges over the list of variables or ojects and lets you access each element of that list, one by one.
      - {{- range}}
      - env. vars are always interpreted as strings even when it is an integer.
        - for that, we can use piping- same concept as UNIX,
          - tool for chaining together template commands
        - {{ .value | quote }}
        - and close the loop with {{- end}}
  
  - helm rendering process:
    - when helm evaluates a chart, it sends all the template files that are defined in the templates directory through helm's template rendering engine.
    - engine then replaces the variables or placeholders in those template files with actual values from the values resources(default values file, user provided values file, or --set option)
  
  - __validating helm chart:__
    - `helm template -f <custom values file> <chart name>`: render chart templates locally and display the output
      - `helm template -f emailservice-values.yaml microsvc`
    - "helm lint" command
      - examines a chart for possible issues
        - ERROR: issues that will cause the chart to fail at installation
        - WARNING: issues that break convention, or recommendations
        - `helm lint -f emailservice-values.yaml microsvc`
    - `helm install --dry-run`: checks generated manifest without installing the chart
      - __difference between --dry-run and template:__
        - --dry-run sends files to k8s cluster, while template only validates it locally
      - `helm install --dry-run -f values/redis-values.yaml redis-cart charts/redis`
  
  - deploy a microservice
    - `helm install -f myvalues.yaml <release name> <chart name>`
      - `helm install -f emailservice-values.yaml emailservice microsvc`
    - `helm ls`
    - `kubectl get pod`

  - create redis helm chart
    - `mkdir charts ; mv microsvc charts/microsvc`
    - `cd charts`
    - `helm create redis`

    ```helm file structure
    .
    ├── charts
    │   ├── microsvc
    │   │   ├── Chart.yaml
    │   │   ├── charts
    │   │   ├── templates
    │   │   │   ├── deployment.yaml
    │   │   │   └── service.yaml
    │   │   └── values.yaml
    │   └── redis
    │       ├── Chart.yaml
    │       ├── charts
    │       ├── templates
    │       │   ├── deployment.yaml
    │       │   └── service.yaml
    │       └── values.yaml
    └── values
        ├── adservice-values.yaml
        ├── cartservice-values.yaml
        ├── checkoutservice-values.yaml
        ├── currencyservice-values.yaml
        ├── emailservice-values.yaml
        ├── frontend-values.yaml
        ├── paymentservice-values.yaml
        ├── productcatalogservice-values.yaml
        ├── recommendationservice-values.yaml
        ├── redis-values.yaml
        └── shippingservice-values.yaml
    ```

    - `chmod u+x install.sh`
    - `./install.sh`
    - this is not an elegant way of working with helm. To uninstall these charts, you would have to run a `helm uninstall` for each release
      - `touch uninstall.sh`
      - `chmod u+x uninstall.sh`
      - `./uninstall.sh`
- __Helmfile:__
  - it is a declarative way of deploying helm charts
  - define the desired state
  - helmfile allows us to declare a definition of an entire k8s cluster in a single yaml file
  - you can define multiple helm releases in it and then change specifications of each release depending on the application or type of environment like dev,testing, production... on which you are deploying your applications.
  - Create Helmfile:
    - `touch helmfile.yaml`
    - you can configure additional values or override any defined values with the helmfile, for example:

    ```helmfile
    releases:
    - name: redis-cart
      chart: charts/redis
      values:
        - values/redis-values.yaml
        - appReplicas: "1"
        - volumeName: "redis-cart-data"
    ```

  - install helmfile
    - install the helmfile tool: `brew install helmfile`
    - deploy helmcharts
      - `helmfile sync`
        - In the background, helmfile will prepare all the releases, screen the helmfile configuration.
        - It then compares the actual state in the cluster with the desired state that is configured in the helmfile
      - `helmfile list`
  - uninstall releases:
    - `helmfile destroy`

  - hosting helmcharts:
    - just like the application code, helm charts and helmfiles are hosted in the git repo as tehy are part of IaaC
      - You can package it and host it with the app code
      - host it in a separate git repo for helm charts
