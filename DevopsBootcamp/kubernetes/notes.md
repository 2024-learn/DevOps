# Kubernetes

- deployment: stateless apps
- statefulSet: stateful apps or dbs

- __worker node:__
- container runtime:
  - also known as container engine
  - it is a software component that can run containers on a host operating system
  - It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.
- kubelet:
  - interacts with both the container and the node
  - starts the pod with the container inside
  - assigns resources from the node to the container like CPU, RAM, and storage resources
- kube-proxy:
  - forwards the requests
- __control Plane:__
- API server:
  - cluster gateway that gets any initial requests of any updatess into the cluster, or queries from the cluster
  - acts as a gatekeeper for authenitcation
- scheduler:
  - after the API server authenticates the request, it forwards it to the scheduler
  - scheduler decides which specific worker node would be able to handle the request of a new pod, component
  - it first looks at your request and sees how many resources the application needs and then matches it with a node that can handle it
  - it only decides on which node the new component should be scheduled, the actual scheduling is done by the kubelet
- controller manager:
  - detects state changes, like crashing pods, and tries to recover the cluster state as soon as possible.
  - it makes a request to the scheduler.
- etcd:
  - a key value store for the cluster state.
  - it is the cluster brain
  - cluster changes, like a crashing pod or a scheduled pod, get stored in the key-value store.
  - the other control plane processes consult it to compare the desired state of the cluster to the present state and therefore try to correct it to achieve the desired state which is stored in the etcd
  - it does not store application data

- kubectl: command line tool for k8s cluster
- install minikube:
  - <https://minikube.sigs.k8s.io/docs/start>
  - `homebrew install minikube`
  - `minikube start --driver docker`
  - `minikube status`

- __Basic kubectl commands:__
  - `kubectl get nodes`
  - `kubectl get pod` or `kubectl get po`
    - ContainerCreating: the pod is created but the pod inside is not yet started
  - `kubectl get services`
  - `kubectl create deployment NAME --image=image --[command] [args...] [options]`
    - `kubectl create deployment nginx-depl --image=nginx`
  - `kubectl get deploy` or `kubectl get deployment`
    - when you create a deployment, the deployment is the blueprint for creating pods
  - `kubectl get replicaset` or `kubectl get rs`
    - RS manages the replicas of a pod.
      - layers of abstraction:
      - deployment manages a replica set, which manages all the replicas of a pod
      - pods are an abstraction of the container
      - everything below a deployment is managed by k8s
  - `kubectl edit deployment nginx-depl` : it produces an autogenerated configuration file with default values that you can edit. As soon as you edit and save the changes in the file, the old pod will be killed and a new one started.
  - `kubectl logs [pod name]`
  - `kubectl describe pod [pod name]`
  - `kubectl exec -it [pod name] -- bin/bash`: enters the pod as a root user
  - `kubectl delete deployment [deploy-name] [deploy-name] ...`
  - `kubectl apply -f [file name]`
  - `kubectl delete -f [file name]`
  - each configuration file has 3 parts:
    - metadata
    - specification
    - status: autogenerated by k8s
  - `kubectl get deployment nginx-depl -o yaml > nginx-depl.yaml` : will direct the result into a file instead of stdout.
  - `kubectl get service` or `kubectl get svc`
  - `kubectl get pod -o wide`: get more information about the pod, including its IP
  - `kubectl get all` : get all components
    - `kubectl get all | grep mongodb`: filter the component you want

- __Secret Configuration file:__
  - kind: Secret
  - metadata/name: a name for your secret
  - type:"opaque"- default for arbitrary key-value pairs
  - data: the actual contents- in key-value pairs
- `echo -n 'username' | base64` will encode plain text 'username' with base64
- `echo -n 'dXNlcm5hbWU=' | base64 -d`: decode base64 text
- the secret has to be created before the deployment for it to be referenced in the deployment using `secretKeyRef`.

- __ConfigMap:__
  - configMap, just like secret has be created in the cluster before referencing it in a deployment
  - it is referenced as `configMapKeyRef` in the deployment file

- Types of Service in Kubernetes: <https://kubernetes.io/docs/concepts/services-networking/service/>
  - ClusterIP: or internal service. this is the default service if type of service is not specified.
    - exposes the service on a cluster-internal IP and makes the service only reacheable from within the cluster
  - LoadBalancer: will also give service an internal IP address.
    - But in addition, it will also give the service an external IP address where the external requests will be coming from
    - in minikube the extrnal IP is not automatically created.
      - `minikube service mongo-express-service` will assign the external service a public IP address
  - NodePort: port range: must be between ports 30000 and 32767
  - ExternalName:

- __Namespaces:__
  - `kubectl get namespace` or `kubectl get ns`

    ``` kubernetes default namespaces
    NAME              STATUS   AGE
    default           Active   2d21h
    kube-node-lease   Active   2d21h
    kube-public       Active   2d21h
    kube-system       Active   2d21h
    ```

  - kube-system:
    - Do not create or modify in this name space.
    - components deployed here are the system processes from the control plane or kubectl processes
  - kube-public:
    - contains publicly accessible data
    - it has a configmap that contains cluster information which is accessible even without authentication
    - `kubectl cluster-info`
  - kube-node-lease:
    - it holds information about the heartbeats of nodes.
    - so each node basically gets its own lease object that contains information about that node's availability
  - default:
    - You can create resources in this namespace(default, of no other namespace defined) or you can create your own namespace
    - `kubectl create namespace <my-namespace>`
    - you can also create the namespace using a configuration file:

    ```namespace confuration file:
    apiVersion: v1
    kind: Namespace
    metadata: my-namepsace
    ```

  - __Why namespaces:__
    - Structure your components: group resources into namespaces.
    - Avoid conflicts between teams, one application: Use namespaces to avaoid one team overwriting the other's application with configuration changes
    - Share services bewteen different environments. resource sharing: consider staging and development. namespaces give the ability to reuse components in both enviroments or in a blue-green deployment
    - access and resource limits on namespace level. Give teams access to their own namespaces and restrict their access to other namespaces where other teams are working. You can also limit the resources like CPU and RAM that one namespace can use
  - __characteristics of a namespace:__
  - You cannot access most resources from another namespace.
    - you would have to define configmaps and secrets for each namespace if they are sharing the same resource.
    - service can be shared across namespaces

    ```access service in another namespace:
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: mongodb-configmap
      namespace: my-namespace
    data:
      database_url: mongodb-service.database  #service.namespace
    ```

  - There are some components that can't be created within a namespace because they live globally on a cluster and cannot be isolated.
    - e.g. volume or persistent volume and node
    - `kubectl api-resources --namespaced=false`: to list all resources that cannot be namespaced
    - `kubectl api-resources --namespaced=true`
  - you can change the active namespace with kubectl:
    - `kubectl config set-context --current --namespace-my-namepsace`
  - you can also use kubens: more convenient, but needs to be installed separately.
  - `brew install kubectx`
  - How to use: <https://github.com/ahmetb/kubectx>

- __Services:__
  - each pod gets its own IP address, but pods are ephemeral. Service has a stable IP address that is not linked to the pod lifecycle
  - it also provides load balancing to pods of the same application/replicas
  - they are a good abstraction of loose coupling for communication within and outside of the cluster
  - a service identifies it member pods or its endpoint pods using the selectors attribute.
  - when you have a pod with two ports, the service knows which port to direct the request to using the targetPort attribute.
  - when you create a service, k8s creates an endpoint object that has the same name as the service itself. K8s will use this endpoints object to keep track of which pods are members/endpoints of the service.
  - service communication: port vs. targetPort-
    - service port is arbitrary
    - targetPort must match the port the container/ application port inside the container is listening at
  - 3 service type attributes: ClusterIp, NodePort, LoadBalancer
  - __ClusterIP:__
    - default service type if no other service is specified.
    - Only accessible within the cluster.
    - Multiport services:
      - when you have multiple ports defined in a service, you have to name those ports, while this is optional in services with only one port.

      ```multiport-service
      apiversion: v1
      kind: Service
      metadata:
        name: mongodb-service
        ...
      spec:
        selector:
          app: mongodb
        ports:
          - name: mongodb
            protocol: TCP
            port: 27017
            targetPort: 27017
          - name: mongodb-exporter
            protocol: TCP
            port: 9216
            targetPort: 9216
      ```

    - Headless Service:
      - used when client/pod wants to communicate with one specific pod directly, instead of a randomly selected pod.
      - Use case: stateful applications like databases
      - client needs to figure out IP addresses of each pod.
        - Option 1: API call to k8s API server and it will return the list of pods and their IP addresses.
          - This makes the app too tied to k8s api and inneficient because you will have to list the pods and their IP addresses everytime you want to connect to the pods
        - Option 2: DNS lookup
          - As an alternative, k8s allows the client to discover pod IP addresses through DNS lookups. When a client performs a DNS lookup for a service, the DNS sserver returns a single IP address which belongs to the service (clusterIP address).
      - however, if you tell k8s that you do not need a cluster IP address of the servicem by setting the clusterIP field to none when creatig a service, then the DNS server will return the pod IP addresses instead of the services IP address. And now the client can do a simple DNS lookup to get the IP address of the pods that are members of that service. and then client can use that IP address to connect to the specific pod it want to talk to or all the pods.

      ```headless service
      apiVersion: v1
      kind: Service
      metadata:
        name: mongo-service-headless
      spec:
        clusterIP: None
        selector:
          app: mongodb
        ports:
          - protocol: TCP
            port: 27017
            targetPort: 27017
      ```

  - __NodePort:__
    - creates a service that is accessible on a static port on each worker node in the cluster.
    - makes the external traffic accessible on the static port on each worker node
    - nodeport value has a predefined range between 30000 and 32767
    - when you create a nodePort service, a cluster Ip is also automatically created.
    - not secure because nodeport service opens the port to directly talk to services on each worker node, so the external clients have access to worker nodes directly.
    - nodeport is not for use in production envs; configure Ingress or LoadBalancer for prod envs.

  - __LoadBalancer:__
    - service is made accessible externally through a cloud provider's load balancer functionality.
    - whenever we create a load balancer service, nodeport and cluster Ip services are created automatically by k8s, to which the external load balancer of the cloud platform will route the traffic to.
    - loadbalancer service is an exttention of NodePort service
    - nodeport service is an extension of clusterIp service
- __Ingress:__
  - List of Ingress Controllers you cna choose from: <https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>
  - external service vs. ingress:
    - Ingress does not work with an external service, it does with an internal service. This means that you don't have to open the application via IP address and port number. Now, if the request arrives from the browser, it is received by the Ingress and then it directs it to the desired service and eventually ends up with the Pod. <https://medium.com/@owaisnasir433/kubernetes-ingress-vs-external-services-c1b61acb9c78#:~:text=To%20differentiate%20more%20clearly%2C%20Ingress,IP%20address%20and%20port%20number.>
    - An ingress is really just a set of rules to pass to a controller that is listening for them. You can deploy a bunch of ingress rules, but nothing will happen unless you have a controller that can process them. A LoadBalancer service could listen for ingress rules, if it is configured to do so.
    - An Ingress Controller is simply a pod that is configured to interpret/ evaluate and process ingress rules and manages all the redirections. One of the most popular ingress controllers supported by kubernetes is nginx. Without it, the ingress would not work.
    - external service:

    ```externalservice.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: myapp-external-service
    spec:
      selector:
        app: myapp
      type: LoadBalancer
      ports:
        - protocol: TCP
          port: 8080
          targetPort: 8080
          nodePort: 35010
    ```

    - ingress:

    ```ingress.yaml
    apiVersion: netwroking.k8s.io/v1
    kind: Ingress
    metadata:
      name: myapp-ingress
    spec:
      rules:
      - host: myapp.com
        http:
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: myapp-internal-service
                  port:
                    number: 8080
    ```

    - routing rules: forward request to the internal service
    - backend: where the incoming request will be directed and the service name should correspond to the internal service name
    - port: internal service port
    - host has to be a valid domain address
      - map the domain name to the IP address, which is the entrypoint to your k8s cluster
  - you can configure the ingress in different ways:
    - using a cloud-provider load balancer which then forwards the request to the internal service
    - using a proxy-server: This is a spearate server with a public IP address and open ports which acts as the entrypoint to the cluster, and none of the servers in k8s cluster is accessible from outside
- install ingress controller in minikube:
  - `minikube addons enable ingress`
  - this automatically starts the k8s nginx implementation of ingress controller
  - minikube uses minikube tunnel to enable ingress access to the resources
- create ingress rule
  - enable minikube dashboard:
    - `minikube dashboard`
  - configure an ingress rule for the dashboard to it is available externally using a domain name
    - `kubectl get all -n kubernetes-dashboard`
    - `kubectl get ingress -n kubernetes-dashboard`
  - map the localhost address to dashboard.com(minikube)
    - 127.0.0.1 dashboard.com
    - `minikube tunnel`: once you close the tunnel, the application will no longer be accessible on the browser although it is configured in the /etc/hosts file
    - now you can accesss the kubernetes dashboard at dashboard.com
- ingress default backend:
  - `kubectl describe ingress -n kubernetes-dashboard`
  - whenever a request comes into the cluster that is not mapped to any backend,so there is no rule for mapping that request to a service, then this default backend is used to handle that request.
  - a good usage for that is to define custom error messages when a page isn't found, when a request comes in that the application cannot handle so that the user still sees a meaningful error message or a custom page where you can redirect them to your home page, etc.

  ```default backend
  spec:
    defaultBackend:
      service:
        name: kubernetes-dashboard
        port:
          number: 80
  ```

- more use cases:
  - defining multiple paths for the same host
    - 1 domain, many services
    - accessible at myapp.com/analytics and myapp.com/shopping

  ```multiple-paths-ingress.yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: simple-fanout-example
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
  spec:
    rules:
    - host: myapp.com
      http:
        paths:
        - pathType: Prefix
          path: /analytics
          backend:
            service:
              name: analytics-service
              port:
                number: 3000
        - pathType: Prefix
          path: /shopping
          backend:
            service:
              name: shopping-service
              port:
                number: 8080
  ```

- multiple sub-domains or domains:
  - insteead of one host and miltiple paths, this one has mutliple hosts with one path, where each host represents  a subdomain and inside each subdomain, there is one path that again redirects the requests to the service in question
  - available at analytics.myapp.com and shopping.myapp.com

  ```multiple-subdomains.yaml
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: name-virtual-host-ingress
  spec:
    rules:
    - host: analytics.myapp.com
      http:
        paths:
        - path: /
          backend:
            service:
              name: analytics-service
              port: 
                number: 3000
    - host: shopping.myapp.com
      http:
        paths:
        - path: /
          backend:
            service:
              name: shopping-service
              port:
                number: 8080
  ```

- configuring a TLS certificate- https://
  - configure a tls attribute under specification:

  ```tls-configuration
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: tls-example-ingress
  spec:
    tls:
    - hosts:
      - myapp.com
      secretName: myapp-secret-tls
    rules:
    - host: myapp.com
      http:
        paths:
        - path: /analytics
          backend:
            service:
              name: myapp-internal-service
              port: 
                number: 8080
  ```

tls-certificate:

```tls-certificate
apiVersion: v1
kind: Secret
metadata:
  name: myapp-secret-tls
  namespace: default
type: kubernetes.io/tls
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
```

- to note:
  - data keys need to be "tls.crt" and 'tls.key"
  - values are file contents, not file paths/locations
  - secret component has to be created in the same namespace as the ingress component for it to be abe to use the secret, otherwise you cannot reference the secret from another namespace

- __Volumes:__
  - we persist data in k8s using volumes
  - storage needs to not depend on the pod lifecycle.
  - storage also needs to be available to all nodes, not just a specific node because we cannot determine the node that the pod will be scheduled on, once it restarts.
  - storage needs to survive even if the cluster crashes
  - persistent volume: A persistent volume is a piece of storage in a cluster that an administrator has provisioned. It is a resource in the cluster, just as a node is a cluster resource. A persistent volume is a volume plug-in that has a lifecycle independent of any individual pod that uses the persistent volume.<https://www.netapp.com/devops-solutions/what-is-kubernetes-persistent-volumes/#:~:text=A%20persistent%20volume%20is%20a,that%20uses%20the%20persistent%20volume.>
  - PersistentVolume(pv) is just an abstract component and it takes the storage from the actual physical storage like the local hard drive from the cluster nodes, or external NFS servers outside teh cluster, cloud storage like AWS block storage, Google cloud storage, etc.
  - you need to create and manage the storage because kubernetes only provides the pv component

  ```pv.yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: mypv
  spec:
    capacity:
      storage: 5Gi
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: slow
    mountOptions:
      - hard
      - nfsvers=4.1
    nfs:
      path: /dir/path/on/nfs/server
      server: nfs-server-ip-address
  ```

- google cloud example:

```gcp-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-volume
  labels:
    topology.kubernetes.io/zone: us-central1-a__us-central1-b
spec:
  capacity:
    storage: 400Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
```

- spec attributes differ according to storage type.
- local storage example:

  ```local-storage.yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-name
  spec:
    capacity:
      storage: 100Gi
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: slow
    local:
      path: /mnt/disks/ssd1
    nodeAffinity:
      required:
        nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - example-node
  ```

- persistent volumes are not namespaced and are available to the whole cluster
- local vs. remote volume types:
  - local volume types violate requirements for data persistence: being tied to 1 specific node and surviving cluster crashes.
  - you should almost always use remote storage for best practices.
  - kubernetes supports several types of volumes: <https://kubernetes.io/docs/concepts/storage/volumes/#volume-types>
- pv resources need to be created before the pod referencing the PV is created.
- The application has to claim that persistent volume storage using a persistent volume claim (pvc).
  - exaample of pvc:

  ```pvc.yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: pvc-name
  spec:
    resources:
      requests:
        storage: 10Gi
    storageClassName: manual
    volumeMode: Filesystem
    accessModes:
      - ReadWriteOnce
  ```

- use that pvc in pod configuration:

```pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: myfrontend
    image: nginx
    volumeMounts: #container
    - mountPath: "/var/www/html"
      name: mypd
  volumes: # pod
    - name: mypd
      persistentVolumeClaim:
        claimName: pvc-name
```

- levels of volume abstractions:
  - pod requests the volume through the pv claim
  - pvc tries to find a volume in the cluster
  - pv has the actual storage backend that it will create that storage from
  - claims must exist in the same namespace as the pods using the pvc, while the pv is not namespaced.
  - Once the pod finds the matching pv, through the pvc, the volume is then mounted into the pod, then the volume is mounted into the container.
  - if you have multiple containers, then you can chose which containers to mount the volume into
- Both configMap and secrets are local volumes that are not create via pv and pvc, but rather own components that are managed by kubernetes itself
- Storage Class:
  - SC provisions persistent volumes dynamically whenever PVC claims it.
    - it creates persistent volumes in the background
  - storage-class example:

``` storageClass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: storage-class-name
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGb: "10"
  fsType: ext4
```

- Each storage backend has its own provisioner.
  - internal provisioner: prefixed with 'kubernetes.io'
  - external provisioner
- storage class usage:
  - requested by persistent volume claim
    - pod clains storage via pvc
    - pvc requests storage from SC
    - SC creates pv that meets the need of the claim

- __ConfigMap and Secret as k8s volumes:__
  - you can create the configmap and secrets as files that can be mounted into the container so that the application in that container can access them.
  
  ```configmap
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: mosquitto-config-file
  data:
    mosquitto.conf: | # file name and below file contents
      log_dest stdout 
      log_type all
      log_timestamp true
      listener 9001
  ```

- secret:

  ```secret
  apiVersion: v1
  kind: Secret
  metadata:
    name: mosquitto-secret-file
  type: Opaque
  data:
    secret.file: |
      ghjJKseyh3CdJLjmn4k= #base64 code value
  ```

- certificate:

```Certificate
  apiVersion: v1
  kind: Secret
  metadata:
    name: my-secret
  type: Opaque
  data:
    cacert.pem: |
      base64 code value of a PEM certificate
```

- code:
  - starting code: <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/configmap-and-secret-volume-types/-/tree/starting-code>
  final code: <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/configmap-and-secret-volume-types>
  - `kubectl exec -it mosquitto-746f66ff55-kwh9g  -- /bin/sh`
  - overwrite the mosquitto.conf file using the configmap by mounting it into the container
  - configmap and secret must be created and exist before the pod starts in the cluster
  - `kubectl get secret`
  - `kubectl get configmap` or `kubectl get cm`
  - mountPath refers to the path in the file system inside teh container

- __statefulSet:__
  - it is a kubernetes component that is used specifically for stateful applications, eg. all databases or any application that stores data to track the state by saving the data in some storage.
  - stateless applications are deployed using deployment; stateful applications are deployed using statefulset component.
  - statefulset makes it possible to replicate the statefulSet app pods.
  - Replica pods are not identical- pod identity. StatefulSer maintains a sticky identity for each of its pods. While the pods are created from the same specification they are not interchangeable.
  - each has a persistent pod identifier that it maintains across any scheduling.
  - the db pods do not have the same physical storage although they use the same data; they each have replicas of the storage that each of them can access for themselves. This means that at any given time that each pod replica must have same data as the other replicas, so they have to continuously synchronize their data. only the main is allowed to change the data and the replicas must know of each change and update their own storage.
  - when a new db pod joins the cluster, it clones the data from not just any pod, but the previous pod, and once that is done and its upto date, it starts continuous synchronization as well to listen for any updates from by the master pod.
  - configure persistent volumes for your statefulset so the data can survive pod and node crashes
  - pod identity: while deployments get a random hash , statefulSet gets a fixed ordered names which is made up of the satefulset name and an ordinal that starts from zero and each additional pod gets the next numeral: $(statefulset name)-$(ordinal), eg mysql-0 (main), mysql-1 ... (replicas)
  - StatefulSet will not create the nextpod in the replica if the previous one is not already up and running. if the first creation fails or is pending, the next replica will not get created at all, it would just wait.
  - the same order is held during deletion, but in reversed order and will wait until the largest ordinal number replica is deleted, before it can delete the one preceding it.
  - each pod also gets its own DNS endpoint from a service. ${pod name}.${governing service domain} eg. mysql-0.svc2, mysql-1.svc2...
  - these two characteristics ( predocatable pod name and fixed individual DNS name) means that when a pod restarts, the IP address will change but the name and endpoint will stay the same- hence sticky identities.
  - The sticky identity ensures that each replica pod can retain its state and its role even when it dies and gets recreated.
  - replicating stateful apps is complex. While k8s helps some, you still need to configure cloning and data synchronization inside the stateful set and also make the remote storage available as well as take care of managing and backing up the remote storage.

- __Helm__
  - package manager for Kubernetes
  - helm charts: bundle of YAML files
  - search for a healm chart: `helm search <keyword>`
  - you can search for helm charts in public registries like artifacthub.io, bitnami
  - there are also helm charts that are available in private registries that are shared within an organization
  - helm is also a templating engine.
    - define a common blueprint
    - dynamic values are replaced by jinja template placeholders (template file)
  - example template yaml config

  ```template-config.yaml
  apiVersion: v1
  kind: pod
  metadata: 
    name: {{ .Values.name }}
  spec:
    containers:
    - name: {{ .Values.container.name }}
      image: {{ .Values.container.image }}
      port: {{ .Values.container.port }}
  ```

- values.yaml

```values.yaml
  name: my-app
  container:
    name: my-app-container
    image: my-app-image
    port: 9001
```

- .Values is an object that is being created based on  the values that are supplied via values.yaml file and also through the CLI with --set flag.
- helm chart structure:
  - top level: myChart/ folder- name of the chart
  - Chart.yaml: meta info about the chart, eg. name, version, dependencies
  - values.yaml: values for the template files. These will be the default values you can override later
  - charts/ folder: chart dependencies
  - templates/ folder: the actual template files or where the template files are stored.
    - when you execute `helm install <chartname>` the template files will be filled with the values from values.yaml
  - you can optionally have other files like README.md or license file, etc
- you can overide the default values in the values.yaml file in two ways:
  - by using the --values flag to represent the new values.yaml file
    - `helm install --values=my-values.yaml <chartname>`
    values.yaml(default)

    ```values.yaml
    imageName: myapp
    port: 8080
    version: 1.0.0
    ```
  
    - my-values.yaml

    ```my-values.yaml
    version: 2.0.0
    ```

    - result:

    ``` result:
    imageName: myapp
    port: 8080
    version: 2.0.0
    ```

  - by using the --set flag on the CLI
    - eg. `helm install --set version=2.0.0`

- Release management:
  - managed by the release binary
  - if an upgrade to a Helm chart is released, or if you need tochange the configuration of your deployment, you can run `helm upgrade <chartname>`
    - any changes made since the last release are going to be applied to the existing deployment instead of creating a new one
  in case the upgrade was fails or the configuration changes are wrong, you can rollback that upgrade using `helm rollback <chartname>` to specific version of the chart if you add the version to the command
- __installing helm:__
  - <https://helm.sh/docs/intro/install>
    - MacOs: `brew install helm`
  - <https://docs.bitnami.com/kubernetes/infrastructure/kubeapps/get-started/install/>
  - <https://gitlab.com/twn-devops-bootcamp/latest/10-kubernetes/helm-demo>

  - `helm repo add bitnami https://charts.bitnami.com/bitnami`
  - `kubectl cluster-info`
  - `helm search repo bitnami`
  - <https://github.com/bitnami/charts/blob/main/bitnami/mongodb>
  - <https://github.com/bitnami/charts/tree/main/bitnami/mongodb>
  - `helm install mongodb --values helm-mongodb.yaml bitnami/mongodb`
  - to delete a chart: `helm delete <my-RELEASE>` ... `helm delete mongodb`
  - `kubectl get secret mongodb -o yaml`
  - <https://medium.com/google-cloud/kubernetes-storage-overview-of-storage-classes-af1102e7b3f9>
  - `kubectl apply -f helm-mongo-express.yaml`
  - `kubectl get svc`, `kubectl get pod`, `kubectl get deploy`
  - install kubernetes ingress controller- nginx via helm chart:
    - `helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx`
    - `helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true`
    - `controller.publishService.enabled=true` will enable a load balancer
  - create ingress rule for mongo-express:
    - `kubectl apply -f helm-ingress.yaml`
    - `kubectl get ingress`
      - accessed at external-ip-address:80 since I have not configured a host
    - `kubectl scale --replicas=0`
    - `kubectl get statefulset`
    - `kubectl scale --replicas=0 statefulset/mongodb`
    - `helm ls`
    - `helm uninstall <chartname>`

- __deploying images in k8s from private Docker repo:__
  - steps to pull image from private registry
    - create secret component: contains credentials for docker registry
      - `aws ecr get-login-password`
    - ssh into minikube:
      - `minikube ssh`
      - `pwd`, `ls -a`
      - `docker login --username AWS -p <the token you got from the aws ecr command above> <AWS-account-id>.dkr.ecr.ca-central-1.amazonaws.com`
    - use the .docker/config.json to create the secret component:
      - copy the file from minikube to the host:
        - `minikube cp minikube:/home/docker/.docker/config.json ~/.docker/config.json`
        - `cat ~/.docker/config.json | base64` and paste this in docker-secret.yaml. Then apply the docker-secret.yaml file to create the secret
        - Alternatively, skip straight to creating the secret:

        ```secret
        kubectl create secret generic my-registry-key \
        --from-file=.dockerconfigjson=~/.docker/config.json \
        --type=kubernetes.io/dockerconfigjson
        ```

      - you can also create the secret like this:

      ```secret
      kubectl create secret docker-registry my-registry-key-two \
      --docker-server=https://137236605350.dkr.ecr.ca-central-1.amazonaws.com \
      --docker-username=AWS \
      --docker-password=<aws ecr get-login-password> \

      ```

    - configure deployment/pod: use secret using imagePullSecrets
    - apply both deployment files.
      - imagePullAlways forces docker to pull the image from AWS
      - imagePullSecrets: references the secret in docker-secret.yaml file
  - *To note!*: Secret has to be in the same namespace as Deployment/StatefulSet or any other component that's being created that needs to reference it.

- __Kubernetes Opperator:__
  - <https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-operators/#:~:text=In%20Kubernetes%2C%20an%20operator%20is,(API)%20and%20kubectl%20tooling.>
  - In Kubernetes, an operator is an application-specific controller that can help you package, deploy, and manage a Kubernetes application.
  - Ordinarily, you run and manage Kubernetes applications via the Kubernetes application programming interface (API) and kubectl tooling. Operators lets you extend the functionality of the Kubernetes API, enabling it to configure, create, and manage instances of applications automatically using a structured process.
  - Operators use the basic capabilities of Kubernetes controllers and resources, but add application-specific or domain knowledge to automate the entire lifecycle of the application it manages.
  - it is a custom control loop in k8s.
  - makes use of (custom resource definitions)CRDs
  - operator takes the basic k8s resources and its controller concept, as a fundation to build upon. on top of that, it includes the domain or application specific knowledge to automate the entire lifecycle of the application it manages or operates
  - while k8s can manage the complete lifecycle of stateless apps, it cannot automate the process natively for stateful apps, and there it uses its own operator for each different application
  - there are already multiple operators that have been created and are ready to use but you can also create your own operator with Operator SDK

- __Role-Based Authorization Control:__
  - least-privilege rule
  - with RBAC, you cna define namespaced permissions. it also determines what you can do with those resources (CRUD)
  - RoleBinding: link("bind) a role to a user or group. All members of the group get thte premissions defined in the role
  - Cluster role defines what resources have what permissions, cluster-wide. cluster role is not namespaced
  - Kubernetes does not manage users natively. it does not have a k8s object for representing normal user accounts. it relies on external sources for creating and managing users and groups
  - Admins can choose from different authentication strategies.
    - can be a static token file, certificates signed by k8s or a 3rd party identity service like ldap
  - admin configures external source
  - API server handles authentication of all the requests
    - pass token file via `--token-auth-file=/users.csv`
    - `kube-apiserver --token-auth-file=/users.csv [other options]`
  - you can also link a service account to a role with roleBinding or to a clusterRole with clusterRoleBinding
  - example role.yaml

  ```role.yaml
  kind: Role
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    # namespace: default
    name: developer
  rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "create", "list"]
    # resourceNames: ["myapp"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
    # resourceNames: ["mydb"]
  ```

  - apiGroups: "" indicates the core API group
  - resources: k8s componenets like pods, deployments, etc
  - verbs: the action on a resource
  - namespace: you can define a namespace, otherwise if the namespace is not defined, then the role will be created in the default namespace
  - resourceNames: you can also define access to only certain pods in that namespace using the resourceNames attribute

  - RoleBinding: attaches or binds a subject to a role

  ```rolebinding
  kind: RoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: phyllis-developer-binding
  subjects:
  - kind: User
    name: phyllis
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: Role
    name: developer
    apiGroup: rbac.authorization.k8s.io
  ```
  
  ```group subject
  subjects:
  - kind: Group
    name: "devops-admins"
    apiGroup: rbac.authorization.k8s.io

  ```

  ```serviceaccount subject
  subjects:
  - kind: ServiceAccount
    name: default
    apiGroup: kube-system
  ```

  - ClusterRole and ClusterRoleBinding example:
    - you can define a cluster role for:
      - cluster-wide resources
      - namespaced resources in a clusterRole: grants access to that component in all namespaces
  
  ```clusterrolebinding for cluster-wide resources
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: cluster-admin
  rules:
  - apiGroups: [""]
    resources: ["nodes", "namespaces"]
    verbs: ["get", "create", "list", "delete", "update"]
  ```

  ```clusterrolebinding for namespaced resources
  kind: ClusterRole
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: cluster-admin
  rules:
  - apiGroups: [""]
    resources: ["pods", "services", "deployments"]
    verbs: ["get", "create", "list", "delete", "update"]
  ```

  - clusterrolebinding:

  ```clusterrolebinding
  kind: ClusterRoleBinding
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    name: read-screts-global
  subjects:
  - kind: Group
    name: cluster-admins
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  ```

  - `kubectl get roles`
  - `kubectl describe role <role name>`

  - You can also checj the priviledges that your current user has using kubectl auth command.
    - kubectl provides `auth can-i` subcommand to check if the current user can perform a given action
      - `kubectl auth can-i create deployments --namespace dev`
    - admins can also check permissions of other users
